Crawler
=======

for each repo:
  clone repo (if necessary)
  fetch changes
  for reach remote branch:
    if branch matches a channel:
      create a commit (if it does not exist)
      for each profile:
        create a build
        
        remove old builds if applicable (TBD)

commit
------
git sha
repo (link)
channel (link)

build
-----
commit
package
profile
status: [new, active, error, success]
dependencies: package[]
missing dependencies: pattern[]

package
-------
name
version
recipe revision
package revision

pattern
-------
name
version

Scheduler
=========

on new commit:
  for each new build:
    try to schedule the build
    
on new package:
  for each failed build with missing dependencies:
    if dependencies can be resolved:
      try to schedule a build
  
Build-Agent
===========

checks out the given commit
create a python scripts which runs the build for the profile
create a docker container with the image from the profile
- map the source directory and 
- map the directory with the python script
configure conan in the docker container
get the package name and version from the docker container and add them to the build
run the python script in the docker container
if no error:
  get the dependencies from the docker container and add them to the build
  run "conan upload <package_name>" in the docker container
  trigger the scheduler
if error:
  get the missing dependencies from the docker container and add them to the build
  add logs etc. to the build (TBD)

  
